{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Francesco0101/Computer-Vision-Project-on-Ground-To-Aerial-Matching/blob/main/CV_project_on_Cross_image_matching.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import"
      ],
      "metadata": {
        "id": "j68I0TkRDOxc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "y9VSTxIBhU6i"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install gdown\n",
        "!pip install rarfile\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install pytorch_lightning"
      ],
      "metadata": {
        "id": "qs909SjRPoJ4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wHBhZ74x6XwB"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install datasets opencv-contrib-python opencv-python ipywidgets scipy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install matplotlib\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FNDG_MB82sw",
        "outputId": "9100d39d-cf26-4e77-b1a4-d66f0f31c3a1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "import zipfile\n",
        "import os\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import pytorch_lightning as pl\n",
        "import csv\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import pytorch_lightning as pl\n",
        "from torch.utils.data import random_split\n",
        "import logging\n",
        "from collections import OrderedDict\n",
        "import torchvision\n",
        "from pytorch_lightning import LightningModule\n",
        "from pytorch_lightning import Trainer\n",
        "from torch import optim\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from pytorch_lightning.callbacks.progress import RichProgressBar\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "pE07XccPTCFg"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#University Dataset da cambiare"
      ],
      "metadata": {
        "id": "FTH52kmbCo0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O file.zip 'https://drive.google.com/file/d/17W9VEPMneRlb6igtSxa--Xh4fSZs3RS_/view?usp=sharing' #Download dataset\n"
      ],
      "metadata": {
        "id": "jLLcFzdGhIRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Percorso del file zip\n",
        "file_zip = \"dataset.zip\"\n",
        "\n",
        "# Percorso di destinazione per l'estrazione\n",
        "percorso_estrazione = \"/content\"\n",
        "\n",
        "# Estrai il file zip\n",
        "with zipfile.ZipFile(file_zip, 'r') as zip_ref:\n",
        "    zip_ref.extractall(percorso_estrazione)\n",
        "\n",
        "# Mostra i file estratti\n",
        "print(\"File estratti in:\", percorso_estrazione)\n",
        "print(\"Elenco dei file estratti:\")\n",
        "for file in os.listdir(percorso_estrazione):\n",
        "    print(file)"
      ],
      "metadata": {
        "id": "iRgfju7XVnpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CVUSA Dataset"
      ],
      "metadata": {
        "id": "zN_O1FwaDMw-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To load directly without drive"
      ],
      "metadata": {
        "id": "9NEGQqWT7S__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "!pip install rarfile\n",
        "!gdown --id 17W9VEPMneRlb6igtSxa--Xh4fSZs3RS_\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "CFPXSjLQSQFx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ff5bc478-e152-4d36-f6cd-41dd62f6584d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n!pip install rarfile\\n!gdown --id 17W9VEPMneRlb6igtSxa--Xh4fSZs3RS_\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import rarfile\n",
        "# import os\n",
        "\n",
        "# # Percorso del file RAR\n",
        "# file_rar = \"/content/drive/MyDrive/CV/CVUSA_subset.rar\"\n",
        "\n",
        "# # Directory di destinazione per l'estrazione\n",
        "# extract_path = \"/content/CVUSA_subset\"\n",
        "\n",
        "# # Estrarre il file RAR\n",
        "# with rarfile.RarFile(file_rar, 'r') as rar:\n",
        "#     # Crea la directory di destinazione se non esiste\n",
        "#     os.makedirs(extract_path, exist_ok=True)\n",
        "#     # Estrai tutto nella directory di destinazione\n",
        "#     rar.extractall(extract_path)\n",
        "\n",
        "# print(\"Estrazione completata.\")"
      ],
      "metadata": {
        "id": "HKdcsd0_DMJc"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GP4np_gE480R",
        "outputId": "ea56cdf3-7048-475b-a2ed-7ba24594851d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Percorsi delle directory\n",
        "#Francesco\n",
        "bingmap_dir = '/content/drive/MyDrive/CVUSA/bingmap'\n",
        "streetview_dir = '/content/drive/MyDrive/CVUSA/streetview'\n",
        "cvusa_dir = '/content/drive/MyDrive/CVUSA'\n",
        "#Simone\n",
        "\n",
        "\n",
        "# Leggi i file dalle directory\n",
        "bingmap_files = sorted(os.listdir(bingmap_dir))\n",
        "streetview_files = sorted(os.listdir(streetview_dir))\n",
        "\n",
        "# Assicurati che ci sia una corrispondenza uno a uno\n",
        "if len(bingmap_files) != len(streetview_files):\n",
        "    raise ValueError(\"Le directory non contengono lo stesso numero di file\")\n",
        "\n",
        "# Percorso del file CSV\n",
        "csv_path = '/content/file_paths.csv'\n",
        "\n",
        "# Scrivi i percorsi dei file nel CSV\n",
        "with open(csv_path, mode='w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    for bingmap_file, streetview_file in zip(bingmap_files, streetview_files):\n",
        "        bingmap_path = os.path.join(bingmap_dir, bingmap_file)\n",
        "        streetview_path = os.path.join(streetview_dir, streetview_file)\n",
        "        writer.writerow([bingmap_path, streetview_path])\n",
        "\n",
        "print(f\"CSV creato con successo in {csv_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_a5caYWNdJZg",
        "outputId": "20bb525d-d265-4c24-d8fb-74a439d45693"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV creato con successo in /content/file_paths.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "XZYwOT8J-zJO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CVUSADatasetPath(Dataset):\n",
        "    def __init__(self, cvusa_dir, directory):\n",
        "        self.directory = directory\n",
        "        self.cvusa_dir = cvusa_dir\n",
        "        self.aerial_paths = []\n",
        "        self.ground_paths = []\n",
        "\n",
        "        with open(self.directory, mode='r') as file:\n",
        "          csv_reader = csv.reader(file, delimiter=',')\n",
        "          for row in csv_reader:\n",
        "              aerial_path = os.path.join(self.cvusa_dir, row[0])\n",
        "              ground_path = os.path.join(self.cvusa_dir, row[1])\n",
        "              if os.path.exists(aerial_path) and os.path.exists(ground_path):\n",
        "                  self.aerial_paths.append(aerial_path)\n",
        "                  self.ground_paths.append(ground_path)\n",
        "              else:\n",
        "                  print(f\"File not found: {aerial_path} or {ground_path}\")\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.aerial_paths) # any image path is good\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.ground_paths[idx], self.aerial_paths[idx]"
      ],
      "metadata": {
        "id": "xKYeh_V7-7Md"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CVUSADataset(pl.LightningDataModule):\n",
        "    def __init__(self, cvusa_dir, directory, batch_size=32):\n",
        "        super().__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.cvusa_dir = cvusa_dir\n",
        "        self.directory = directory\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        self.dataset = CVUSADatasetPath(self.cvusa_dir, self.directory)\n",
        "        self.train_dataset, self.val_dataset, self.test_dataset = random_split(self.dataset, [0.8, 0.1, 0.1])\n",
        "\n",
        "    def transform(self, set_type):\n",
        "      transform = None\n",
        "      if set_type == 'train':\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.0, 0.0, 0.0), (1.0, 1.0, 1.0))\n",
        "        ])\n",
        "      else:\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.0, 0.0, 0.0), (1.0, 1.0, 1.0))\n",
        "        ])\n",
        "      return transform\n",
        "\n",
        "\n",
        "    def load_images(self, paths, set_type):\n",
        "      images = []\n",
        "      idx = []\n",
        "      for i in range(len(paths)):\n",
        "        img = cv2.imread(paths[i])\n",
        "        img = Image.fromarray(img)\n",
        "        img = self.transform(set_type)(img)\n",
        "        images.append(img)\n",
        "        idx.append(paths[i])\n",
        "      return torch.stack(images)\n",
        "\n",
        "    def collate_fn_train(self, batch):\n",
        "      return self.collate_fn(batch, 'train')\n",
        "\n",
        "    def collate_fn_val(self, batch):\n",
        "      return self.collate_fn(batch, 'val')\n",
        "\n",
        "    def collate_fn(self, batch, set_type):\n",
        "      ground_paths, aerial_paths = zip(*batch)\n",
        "      ground_images= self.load_images(ground_paths, set_type)\n",
        "      aerial_images= self.load_images(aerial_paths, set_type)\n",
        "      return ground_images, aerial_images\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, collate_fn = self.collate_fn_train, num_workers=2)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, collate_fn = self.collate_fn_val, num_workers=2)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False, collate_fn = self.collate_fn_val, num_workers=2)\n",
        "\n"
      ],
      "metadata": {
        "id": "sEgrFQwAIIo1"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Siamese Branches"
      ],
      "metadata": {
        "id": "208opFqWJDYo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###VGG"
      ],
      "metadata": {
        "id": "YImTHx7zNk7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models\n",
        "\n",
        "class SiameseNetworkVGG(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SiameseNetworkVGG, self).__init__()\n",
        "\n",
        "        # Load the pretrained VGG16 model\n",
        "        vgg16 = models.vgg16(pretrained=True)\n",
        "        self.vgg16_features = vgg16.features\n",
        "        # print(\"features: \", vgg16.features)  #feature vgg\n",
        "\n",
        "        # Freeze the parameters of all layers except the last block\n",
        "        for param in self.vgg16_features[:24].parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Define fully connected layers\n",
        "        self.fc1 = nn.Linear(512 * 7 * 7, 512)  # Adjust input size based on VGG16 output\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "\n",
        "    def forward_one(self, x):\n",
        "        # print(\"x prima di vgg: \", x.size())\n",
        "        x = self.vgg16_features(x)\n",
        "        # print(\"x: \", x.size())\n",
        "        x = F.adaptive_avg_pool2d(x, (7, 7))\n",
        "        # print(\"x dopo avg_pool: \",x.size())\n",
        "        x = x.view(x.size()[0], -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, input1, input2):\n",
        "        grd_global = self.forward_one(input1)\n",
        "        sat_global = self.forward_one(input2)\n",
        "        return grd_global, sat_global\n",
        "\n"
      ],
      "metadata": {
        "id": "-c2FAN8IJFMl"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##RESNET BRANCH"
      ],
      "metadata": {
        "id": "wdMTOVNGOdLR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "\n",
        "class SiameseNetworkRESNET(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SiameseNetworkRESNET, self).__init__()\n",
        "\n",
        "        # Load the pretrained ResNet-50 model with the best available weights\n",
        "        resnet = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
        "\n",
        "        # Extract the feature extraction part of ResNet-50 (remove the last two layers)\n",
        "        self.resnet_features = nn.Sequential(*list(resnet.children())[:-2])\n",
        "\n",
        "        # Optionally freeze some of the ResNet layers\n",
        "        for param in list(self.resnet_features.parameters())[:-3]:\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Define fully connected layers\n",
        "        self.fc1 = nn.Linear(2048 * 7 * 7, 512)  # Adjust input size based on ResNet-50 output\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "\n",
        "    def forward_one(self, x):\n",
        "        x = self.resnet_features(x)\n",
        "        x = F.adaptive_avg_pool2d(x, (7, 7))  # Ensure the correct input size to the fc1 layer\n",
        "        x = x.view(x.size()[0], -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, input1, input2):\n",
        "        grd_global = self.forward_one(input1)\n",
        "        sat_global = self.forward_one(input2)\n",
        "        return grd_global, sat_global\n"
      ],
      "metadata": {
        "id": "ReH4gbkjOfkm"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TRANSFORMER BRANCH"
      ],
      "metadata": {
        "id": "n8LvyWcZT6Sy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ViTImageProcessor, ViTModel\n",
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "# processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "# model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "# inputs = processor(images=image, return_tensors=\"pt\")\n",
        "\n",
        "# outputs = model(**inputs)\n",
        "# last_hidden_states = outputs.last_hidden_state"
      ],
      "metadata": {
        "id": "4umH1jS9bKWz"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "\n",
        "class SiameseNetworkVIT(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SiameseNetworkVIT, self).__init__()\n",
        "\n",
        "        self.processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "        self.model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "        # self.model.train()\n",
        "        # Define fully connected layers\n",
        "        self.fc1 = nn.Linear(151296, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "\n",
        "    def forward_one(self, x):\n",
        "        print(\"x prima: \",x.size())\n",
        "        x = self.processor(images=x, return_tensors=\"pt\")\n",
        "        print(\"x dopo processor: \",x[\"pixel_values\"].size())\n",
        "        outputs = self.model(**x)\n",
        "        x = outputs.last_hidden_state\n",
        "        print(\"x last_hidden state: \",x.size())\n",
        "        x = x.view(x.size()[0], -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        print(\"x finale: \",x.size())\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, input1, input2):\n",
        "        grd_global = self.forward_one(input1)\n",
        "        sat_global = self.forward_one(input2)\n",
        "        return grd_global , sat_global\n",
        "\n",
        "# Example usage:\n",
        "# model = SiameseNetwork()\n",
        "# output = model(input1, input2)\n"
      ],
      "metadata": {
        "id": "08iponz_T9Mh"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##GENERAL NETWORK"
      ],
      "metadata": {
        "id": "bgpcIFgJOgAi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SiameseNetworkLightning(pl.LightningModule):\n",
        "    def __init__(self, learning_rate=1e-3, branch = \"VGG\"):\n",
        "        super(SiameseNetworkLightning, self).__init__()\n",
        "        self.branch = branch\n",
        "        if branch == \"VGG\":\n",
        "          self.model = SiameseNetworkVGG()\n",
        "        elif branch == \"RESNET\":\n",
        "          self.model = SiameseNetworkRESNET()\n",
        "        elif branch == \"VIT\":\n",
        "          self.model = SiameseNetworkVIT()\n",
        "        else:\n",
        "          raise ValueError(\"Branch must be either 'VGG' or 'RESNET'\")\n",
        "\n",
        "        self.learning_rate = learning_rate\n",
        "        self.loss_weight = 10.0\n",
        "\n",
        "    def loss(self,triplet_dist_g2s , triplet_dist_s2g , pair_n ):\n",
        "        loss_g2s = torch.sum(torch.log(1 + torch.exp(triplet_dist_g2s * self.loss_weight))) / pair_n\n",
        "        loss_s2g = torch.sum(torch.log(1 + torch.exp(triplet_dist_s2g * self.loss_weight))) / pair_n\n",
        "        loss = (loss_g2s + loss_s2g) / 2.0\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def validate(self, grd_descriptor, sat_descriptor):\n",
        "        accuracy = 0.0\n",
        "        data_amount = 0.0\n",
        "        dist_array = 2 - 2 * np.matmul(sat_descriptor.cpu(), np.transpose(grd_descriptor.cpu()))\n",
        "        top1_percent = int(dist_array.shape[0] * 0.01) + 1\n",
        "        for i in range(dist_array.shape[0]):\n",
        "            gt_dist = dist_array[i, i]\n",
        "            prediction = torch.sum(dist_array[:, i].cpu().lt(gt_dist.cpu()))\n",
        "            # print(\"la pred: \",prediction)\n",
        "            if prediction < top1_percent:\n",
        "                accuracy += 1.0\n",
        "            data_amount += 1.0\n",
        "        accuracy /= data_amount\n",
        "\n",
        "        return accuracy\n",
        "\n",
        "    def validate_topk(self, dist_array, topK):\n",
        "        accuracy = 0.0\n",
        "        data_amount = 0.0\n",
        "\n",
        "        for i in range(dist_array.shape[0]):\n",
        "            gt_dist = dist_array[i, i]\n",
        "            prediction = np.sum(dist_array[i, :] < gt_dist)\n",
        "            if prediction < topK:\n",
        "                accuracy += 1.0\n",
        "            data_amount += 1.0\n",
        "        accuracy /= data_amount\n",
        "\n",
        "        return accuracy\n",
        "\n",
        "    # def search(self, grd_descriptor, sat_descriptor, grd_data , sat_data ,topK):\n",
        "\n",
        "    #     dist_array = 2 - 2 * np.matmul(sat_descriptor.cpu(), np.transpose(grd_descriptor.cpu()))\n",
        "    #     topk_indices = []\n",
        "\n",
        "    #     for i in range(dist_array.shape[0]):\n",
        "    #         gt_dist = dist_array[i, i]\n",
        "    #         distances = dist_array[i, :]\n",
        "    #         distances[i] = np.inf  # Escludi la distanza con se stesso\n",
        "\n",
        "    #         # Trova gli indici delle top-K immagini (con distanze minori di gt_dist)\n",
        "    #         topk_indices_i = np.argsort(distances)[:topK]\n",
        "    #         topk_indices.append(topk_indices_i)\n",
        "\n",
        "    #     print(\"topk indices: \",topk_indices)\n",
        "\n",
        "    #     for i in range(len(grd_data)):\n",
        "    #         print(\"indici: \",topk_indices[i])\n",
        "    #         print(\"immagine originale: \")\n",
        "    #         self.imshow(grd_data[i])\n",
        "    #         for j in range(topK):\n",
        "    #           print(j,\"più vicina\")\n",
        "    #           self.imshow(sat_data[topk_indices[i][j]])\n",
        "    #           print(\"ground della\",i,\" più vicina: \")\n",
        "    #           self.imshow(grd_data[topk_indices[i][j]])\n",
        "\n",
        "    #     return None\n",
        "\n",
        "\n",
        "    def imshow(self, img):\n",
        "        img = img / 2 + 0.5  # De-normalizzare\n",
        "        npimg = img.numpy()\n",
        "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "        plt.show()\n",
        "\n",
        "    def forward(self, ground, aerial):\n",
        "\n",
        "        grd_global, sat_global =self.model(ground, aerial)\n",
        "\n",
        "        dist_matrix = 2 - 2 * torch.matmul(sat_global, grd_global.t())\n",
        "\n",
        "        pos_dist = torch.diag(dist_matrix)\n",
        "\n",
        "        batch_size = sat_global.size(0)\n",
        "\n",
        "        pair_n = batch_size * (batch_size - 1.0)\n",
        "\n",
        "        # ground to satellite\n",
        "        triplet_dist_g2s = pos_dist - dist_matrix\n",
        "\n",
        "        # satellite to ground\n",
        "        triplet_dist_s2g = pos_dist.unsqueeze(1) - dist_matrix\n",
        "\n",
        "        return triplet_dist_g2s, triplet_dist_s2g, pair_n, grd_global, sat_global\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "      grd, sat = batch\n",
        "      dist_g2s , dist_s2g , pair_n , grd_global , sat_global = self.forward(grd ,sat)\n",
        "      loss_val = self.loss(dist_g2s , dist_s2g , pair_n)\n",
        "      self.log('train_loss', loss_val, on_step=False, on_epoch=True, prog_bar=True )\n",
        "      tqdm_dict = OrderedDict({\"loss_train\": loss_val})\n",
        "      output = {\"loss\": loss_val, \"progress_bar\": tqdm_dict, \"log\": tqdm_dict}\n",
        "      return output\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        grd, sat = batch\n",
        "        dist_g2s, dist_s2g, pair_n, grd_global , sat_global = self.forward(grd, sat)\n",
        "        loss_val = self.loss(dist_g2s, dist_s2g, pair_n)\n",
        "        # self.search(grd_global, sat_global, grd, sat, 2)\n",
        "        accuracy_val = self.validate(grd_global, sat_global)  # Calcola l'accuratezza\n",
        "        self.log('val_loss', loss_val, on_step=False, on_epoch=True, prog_bar=True )\n",
        "        self.log('val_accuracy', accuracy_val, on_step=False, on_epoch=True, prog_bar=True )\n",
        "        output= {\"val_loss\": loss_val, \"val_accuracy\": accuracy_val}\n",
        "        return output\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        grd, sat = batch\n",
        "        dist_g2s, dist_s2g, pair_n, grd_global, sat_global = self.forward(grd, sat)\n",
        "        loss_val = self.loss(dist_g2s, dist_s2g, pair_n)\n",
        "        accuracy_val = self.validate(grd_global, sat_global)  # Calcola l'accuratezza\n",
        "\n",
        "        self.log('test_loss', loss_val, on_step=False, on_epoch=True, prog_bar=True )\n",
        "        self.log('test_accuracy', accuracy_val, on_step=False, on_epoch=True, prog_bar=True )\n",
        "        return {\"test_loss\": loss_val, \"test_accuracy\": accuracy_val}\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
        "        return optimizer\n"
      ],
      "metadata": {
        "id": "oabbPLlqT1gx"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "directory = \"/content/file_paths.csv\"\n",
        "batch_size = 8\n",
        "data_module = CVUSADataset(cvusa_dir, directory, batch_size)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # De-normalizzare\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# Iterare attraverso il DataLoader e stampare i batch\n",
        "for batch_idx, (ground_images, aerial_images, idx_g, idx_a) in enumerate(train_loader):\n",
        "    print(f\"Batch {batch_idx + 1}\")\n",
        "    print(\"Ground images: \", idx_g[0])\n",
        "    imshow(ground_images[0])\n",
        "    print(\"Aerial images:\", idx_a[0])\n",
        "    imshow(aerial_images[0]) \"\"\"\n",
        "\n",
        "\n",
        "    # Limit the number of printed batches for readability\n",
        "model = SiameseNetworkLightning(branch=\"VGG\")\n",
        "trainer = Trainer(max_epochs=10)\n",
        "# trainer.fit(model, data_module)\n",
        "trainer.validate(model, data_module)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952,
          "referenced_widgets": [
            "ee6f2375611e4cdbb7c5a3dd2162a0b9",
            "f8a158b442764544b15783f465418a5d",
            "4fd876bd8b7a4231831cb3a415497c66",
            "bf7ae107b04d4d3b8340554a8eac3843",
            "5c9357bae738484398db5818e6e0d83e",
            "af5ccafa22344e83b55345382694266c",
            "9500a11cec4d4fda8abc4b45d33ec8d3",
            "29231e44ba814452a67ce34f51c9eb53",
            "5fb8b25402644115a453d947a1d21b8f",
            "bdfc4834086c4061be8b830d1932a5bb",
            "b4765129894d460fbf95ae643fa306d2"
          ]
        },
        "id": "71PHcx1AV10F",
        "outputId": "11d4d761-398a-4fa6-b57a-f3a640f7d88e"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ee6f2375611e4cdbb7c5a3dd2162a0b9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "top1 =  1\n",
            "la pred:  tensor(1)\n",
            "top1 =  1\n",
            "la pred:  tensor(4)\n",
            "top1 =  1\n",
            "la pred:  tensor(6)\n",
            "top1 =  1\n",
            "la pred:  tensor(7)\n",
            "top1 =  1\n",
            "la pred:  tensor(1)\n",
            "top1 =  1\n",
            "la pred:  tensor(2)\n",
            "top1 =  1\n",
            "la pred:  tensor(6)\n",
            "top1 =  1\n",
            "la pred:  tensor(0)\n",
            "top1 =  1\n",
            "la pred:  tensor(4)\n",
            "top1 =  1\n",
            "la pred:  tensor(5)\n",
            "top1 =  1\n",
            "la pred:  tensor(2)\n",
            "top1 =  1\n",
            "la pred:  tensor(0)\n",
            "top1 =  1\n",
            "la pred:  tensor(3)\n",
            "top1 =  1\n",
            "la pred:  tensor(7)\n",
            "top1 =  1\n",
            "la pred:  tensor(1)\n",
            "top1 =  1\n",
            "la pred:  tensor(2)\n",
            "top1 =  1\n",
            "la pred:  tensor(4)\n",
            "top1 =  1\n",
            "la pred:  tensor(0)\n",
            "top1 =  1\n",
            "la pred:  tensor(7)\n",
            "top1 =  1\n",
            "la pred:  tensor(0)\n",
            "top1 =  1\n",
            "la pred:  tensor(7)\n",
            "top1 =  1\n",
            "la pred:  tensor(4)\n",
            "top1 =  1\n",
            "la pred:  tensor(4)\n",
            "top1 =  1\n",
            "la pred:  tensor(1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Search"
      ],
      "metadata": {
        "id": "-AuLAdBeryMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  # def search(self, grd_descriptor, sat_descriptor,topK):\n",
        "\n",
        "  #       dist_array = 2 - 2 * np.matmul(sat_descriptor.cpu(), np.transpose(grd_descriptor.cpu()))\n",
        "  #       topk_indices = []\n",
        "\n",
        "  #       for i in range(dist_array.shape[0]):\n",
        "  #           gt_dist = dist_array[i, i]\n",
        "  #           distances = dist_array[i, :]\n",
        "  #           distances[i] = np.inf  # Escludi la distanza con se stesso\n",
        "\n",
        "  #           # Trova gli indici delle top-K immagini (con distanze minori di gt_dist)\n",
        "  #           topk_indices_i = np.argsort(distances)[:topK]\n",
        "  #           topk_indices.append(topk_indices_i)\n",
        "\n",
        "  #       return topk_indices"
      ],
      "metadata": {
        "id": "vHTQ9xGEtTEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SIFT APPROACH"
      ],
      "metadata": {
        "id": "YHv90khTDZ6D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##UNIVERSITY"
      ],
      "metadata": {
        "id": "jOInv_qCa9c4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###GROUND"
      ],
      "metadata": {
        "id": "a0CzJwB8bGgs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FGDhV8ee7hwv"
      },
      "outputs": [],
      "source": [
        "# Definisci il percorso della cartella contenente le sottocartelle con le immagini\n",
        "source_dir = \"/content/University-Release/train/street\"\n",
        "\n",
        "# Dizionario per contenere le immagini\n",
        "data = {}\n",
        "\n",
        "# Itera su tutte le sottocartelle nella cartella di origine\n",
        "for i, folder_name in enumerate(sorted(os.listdir(source_dir))):\n",
        "    # Crea il percorso completo della sottocartella\n",
        "    folder_path = os.path.join(source_dir, folder_name)\n",
        "    # Se la sottocartella è una cartella\n",
        "    if os.path.isdir(folder_path):\n",
        "        # Lista per contenere le immagini della cartella corrente\n",
        "        folder_images = []\n",
        "        # Itera su tutti i file all'interno della sottocartella\n",
        "        for filename in sorted(os.listdir(folder_path)):\n",
        "            # Crea il percorso completo del file\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "            # Carica l'immagine\n",
        "            image = cv2.imread(file_path)\n",
        "            # Aggiungi l'immagine alla lista delle immagini della cartella corrente\n",
        "            folder_images.append(image)\n",
        "        # Aggiungi le immagini della cartella al dizionario data\n",
        "        data[i] = folder_images\n",
        "\n",
        "# Visualizza il risultato\n",
        "for folder_id, images in data.items():\n",
        "    print(f\"Folder ID: {folder_id}, Numero di immagini: {len(images)}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R3o9xx1f-PpQ"
      },
      "outputs": [],
      "source": [
        "cv2_imshow(data[0][0])\n",
        "print(data[1][1].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfILW83ZCwna"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_A2yY1vc-rUJ"
      },
      "outputs": [],
      "source": [
        "images_training = []\n",
        "for images in data.values():\n",
        "  for image in images:\n",
        "    images_training.append(image)\n",
        "print(len(images_training))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "cv2_imshow(images_training[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1YSXbKX9eIoC"
      },
      "outputs": [],
      "source": [
        "# convert images to grayscale\n",
        "bw_images_g = []\n",
        "for img in tqdm(images_training):\n",
        "    # if RGB, transform into grayscale\n",
        "    if len(img.shape) == 3:\n",
        "        bw_images_g.append(cv2.cvtColor(img, cv2.COLOR_BGR2GRAY))\n",
        "    else:\n",
        "        # if grayscale, do not transform\n",
        "        bw_images_g.append(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbFJVP68eLmV"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.imshow(bw_images[1], cmap='gray')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6uZwi-YmePGm"
      },
      "outputs": [],
      "source": [
        "# defining feature extractor that we want to use (SIFT)\n",
        "extractor = cv2.xfeatures2d.SIFT_create()\n",
        "\n",
        "# initialize lists where we will store *all* keypoints and descriptors\n",
        "keypoints = []\n",
        "descriptors = []\n",
        "\n",
        "for img in tqdm(bw_images):\n",
        "    # extract keypoints and descriptors for each image\n",
        "    img_keypoints, img_descriptors = extractor.detectAndCompute(img, None)\n",
        "    keypoints.append(img_keypoints)\n",
        "    descriptors.append(img_descriptors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vM4mHTqCePnx"
      },
      "outputs": [],
      "source": [
        "print(f\"len before: {len(descriptors)}\")\n",
        "# initialize list to store idx values of records to drop\n",
        "to_drop = []\n",
        "for i, img_descriptors in enumerate(descriptors):\n",
        "    # if there are no descriptors, add record idx to drop list\n",
        "    if img_descriptors is None:\n",
        "        to_drop.append(i)\n",
        "\n",
        "print(f\"indexes: {to_drop}\")\n",
        "# delete from list in reverse order\n",
        "for i in sorted(to_drop, reverse=True):\n",
        "    del descriptors[i], keypoints[i]\n",
        "\n",
        "print(f\"len after: {len(descriptors)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p03AcfPAeS5d"
      },
      "outputs": [],
      "source": [
        "output_image = []\n",
        "for x in range(3):\n",
        "    output_image.append(cv2.drawKeypoints(bw_images[x], keypoints[x], 0, (255, 0, 0),\n",
        "                                 flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS))\n",
        "    plt.imshow(output_image[x], cmap='gray')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDS2MIA2eXam"
      },
      "outputs": [],
      "source": [
        "# select the same numbers in each run\n",
        "np.random.seed(0)\n",
        "# select 1000 random image index values\n",
        "sample_idx = np.random.randint(0, len(data)+1, 1000).tolist()\n",
        "len(sample_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxcNGG9_eZcl"
      },
      "outputs": [],
      "source": [
        "# extract the sample from descriptors\n",
        "# (we don't need keypoints)\n",
        "descriptors_sample = []\n",
        "\n",
        "for n in tqdm(sample_idx):\n",
        "    descriptors_sample.append(np.array(descriptors[n]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYb69Ff9ecjF"
      },
      "outputs": [],
      "source": [
        "all_descriptors = []\n",
        "# extract image descriptor lists\n",
        "for img_descriptors in tqdm(descriptors_sample):\n",
        "    # extract specific descriptors within the image\n",
        "    for descriptor in img_descriptors:\n",
        "        all_descriptors.append(descriptor)\n",
        "# convert to single numpy array\n",
        "all_descriptors = np.stack(all_descriptors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMCbY1RKeeXp"
      },
      "outputs": [],
      "source": [
        "# check the shape\n",
        "all_descriptors.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cx-aN_elegaG"
      },
      "outputs": [],
      "source": [
        "# we can count the number of descriptors contained in descriptors to confirm\n",
        "count = []\n",
        "for img_descriptors in descriptors_sample:\n",
        "    count.append(len(img_descriptors))\n",
        "# here we can see the number of descriptors for the first five images\n",
        "print(f\"first five: {count[:5]}\")\n",
        "# and if we sum them all, we should see the 39893 from before\n",
        "print(f\"count all: {sum(count)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZpbDuvceitP"
      },
      "outputs": [],
      "source": [
        "# perform k-means clustering to build the codebook\n",
        "from scipy.cluster.vq import kmeans\n",
        "\n",
        "k = 200\n",
        "iters = 1\n",
        "codebook, variance = kmeans(all_descriptors, k, iters) # It takes approx. 2-3 minutes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bhr8ILSTenC8"
      },
      "outputs": [],
      "source": [
        "# vector quantization\n",
        "from scipy.cluster.vq import vq\n",
        "\n",
        "visual_words = []\n",
        "for img_descriptors in tqdm(descriptors):\n",
        "    # for each image, map each descriptor to the nearest codebook entry\n",
        "    img_visual_words, distance = vq(img_descriptors, codebook)\n",
        "    visual_words.append(img_visual_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pgRR1sxseo3e"
      },
      "outputs": [],
      "source": [
        "# let's see what the visual words look like for image 0\n",
        "visual_words[0][:5], len(visual_words[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NhauGjf7etd2"
      },
      "outputs": [],
      "source": [
        "frequency_vectors = []\n",
        "for img_visual_words in tqdm(visual_words):\n",
        "    # create a frequency vector for each image\n",
        "    img_frequency_vector = np.zeros(k)\n",
        "    for word in img_visual_words:\n",
        "        img_frequency_vector[word] += 1\n",
        "    frequency_vectors.append(img_frequency_vector)\n",
        "# stack together in numpy array\n",
        "frequency_vectors = np.stack(frequency_vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vt_K9NsuevBj"
      },
      "outputs": [],
      "source": [
        "frequency_vectors.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fh3XnjL8ew9l"
      },
      "outputs": [],
      "source": [
        "# we know from above that ids 84, 22, 45, and 172 appear in image 0\n",
        "for i in [84,  22,  45, 172]:\n",
        "    print(f\"{i}: {frequency_vectors[0][i]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1noFCx-Eeyt5"
      },
      "outputs": [],
      "source": [
        "frequency_vectors[0][:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjiYnAvde0LO"
      },
      "outputs": [],
      "source": [
        "plt.bar(list(range(k)), frequency_vectors[0])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17IxXbL8e0rO"
      },
      "outputs": [],
      "source": [
        "# N is the number of images, i.e. the size of the dataset\n",
        "N = 2659\n",
        "\n",
        "# df is the number of images that a visual word appears in\n",
        "# we calculate it by counting non-zero values as 1 and summing\n",
        "df = np.sum(frequency_vectors > 0, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9SaS2YL8fEcA"
      },
      "outputs": [],
      "source": [
        "df.shape, df[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izWvkZ3qfF_g"
      },
      "outputs": [],
      "source": [
        "idf = np.log(N/ df)\n",
        "idf.shape, idf[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Icnwu07fIlk"
      },
      "outputs": [],
      "source": [
        "tfidf = frequency_vectors * idf\n",
        "tfidf.shape, tfidf[0][:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YECNCnmKfKYh"
      },
      "outputs": [],
      "source": [
        "plt.bar(list(range(k)), tfidf[0])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WKIJ_4P3fK3Z"
      },
      "outputs": [],
      "source": [
        "search_i = 0\n",
        "\n",
        "plt.imshow(bw_images[search_i], cmap='gray')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwtP64VDfPmG"
      },
      "outputs": [],
      "source": [
        "# cosine similarity\n",
        "from numpy.linalg import norm\n",
        "\n",
        "a = tfidf[search_i]\n",
        "b = tfidf  # set search space to the full sample\n",
        "\n",
        "cosine_similarity = np.dot(a, b.T)/(norm(a) * norm(b, axis=1))\n",
        "print(\"Min cosine similarity:\", round(np.min(cosine_similarity),1))\n",
        "print(\"Max cosine similarity:\", np.max(cosine_similarity))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-x70icggfRMR"
      },
      "outputs": [],
      "source": [
        "cosine_similarity.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_unEIJ5fSmc"
      },
      "outputs": [],
      "source": [
        "cosine_similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qws74Q1ffUUc"
      },
      "outputs": [],
      "source": [
        "top_k = 5\n",
        "idx = np.argsort(-cosine_similarity)[:top_k]\n",
        "idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93AjBD6SfV4d"
      },
      "outputs": [],
      "source": [
        "cosine_similarity[idx[0]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUraJ5dWfYAu"
      },
      "outputs": [],
      "source": [
        "for i in idx:\n",
        "    print(f\"{i}: {round(cosine_similarity[i], 4)}\")\n",
        "    plt.imshow(bw_images[i], cmap='gray')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRcw9NzhfZ8Y"
      },
      "outputs": [],
      "source": [
        "def search(i: int, top_k: int = 5):\n",
        "    print(\"Search image:\")\n",
        "    # show the search image\n",
        "    plt.imshow(bw_images[i], cmap='gray')\n",
        "    plt.show()\n",
        "    print(\"-----------------------------------------------------\")\n",
        "    # get search image vector\n",
        "    a = tfidf[i]\n",
        "    # get the cosine distance for the search image `a`\n",
        "    cosine_similarity = np.dot(a, b.T)/(norm(a) * norm(b, axis=1))\n",
        "    # get the top k indices for most similar vecs\n",
        "    idx = np.argsort(-cosine_similarity)[:top_k]\n",
        "    # display the results\n",
        "    for i in idx:\n",
        "        print(f\"{i}: {round(cosine_similarity[i], 4)}\")\n",
        "        plt.imshow(bw_images[i], cmap='gray')\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77qNrLrufbk9"
      },
      "outputs": [],
      "source": [
        "search(121)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uaanl3zTfc8h"
      },
      "outputs": [],
      "source": [
        "search(3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(3,7):\n",
        "  cv2_imshow(images_training[i])"
      ],
      "metadata": {
        "id": "0_JVS0yHmczh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0KN0Rj_feFo"
      },
      "outputs": [],
      "source": [
        "search(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1W_LVSIQpRBJ"
      },
      "source": [
        "### AERIAL"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Definisci il percorso della cartella contenente le sottocartelle con le immagini\n",
        "source_dir = \"/content/University-Release/train/satellite\"\n",
        "\n",
        "# Dizionario per contenere le immagini\n",
        "data = {}\n",
        "\n",
        "# Itera su tutte le sottocartelle nella cartella di origine\n",
        "for i, folder_name in enumerate(sorted(os.listdir(source_dir))):\n",
        "    # Crea il percorso completo della sottocartella\n",
        "    folder_path = os.path.join(source_dir, folder_name)\n",
        "    # Se la sottocartella è una cartella\n",
        "    if os.path.isdir(folder_path):\n",
        "        # Lista per contenere le immagini della cartella corrente\n",
        "        folder_images = []\n",
        "        # Itera su tutti i file all'interno della sottocartella\n",
        "        for filename in sorted(os.listdir(folder_path)):\n",
        "            # Crea il percorso completo del file\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "            # Carica l'immagine\n",
        "            image = cv2.imread(file_path)\n",
        "            # Aggiungi l'immagine alla lista delle immagini della cartella corrente\n",
        "            folder_images.append(image)\n",
        "        # Aggiungi le immagini della cartella al dizionario data\n",
        "        data[i] = folder_images\n",
        "\n",
        "# Visualizza il risultato\n",
        "for folder_id, images in data.items():\n",
        "    print(f\"Folder ID: {folder_id}, Numero di immagini: {len(images)}\")"
      ],
      "metadata": {
        "id": "ChmjVGkSpsoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZACc_U2npRBP"
      },
      "outputs": [],
      "source": [
        "images_training = []\n",
        "for images in data.values():\n",
        "  for image in images:\n",
        "    images_training.append(image)\n",
        "print(len(images_training))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "cv2_imshow(images_training[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhou3n4PpRBQ"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "\n",
        "# convert images to grayscale\n",
        "bw_images = []\n",
        "for img in tqdm(images_training):\n",
        "    # if RGB, transform into grayscale\n",
        "    if len(img.shape) == 3:\n",
        "        bw_images.append(cv2.cvtColor(img, cv2.COLOR_BGR2GRAY))\n",
        "    else:\n",
        "        # if grayscale, do not transform\n",
        "        bw_images.append(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwnlJziqpRBQ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.imshow(bw_images[1], cmap='gray')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T32FHbv-pRBQ"
      },
      "outputs": [],
      "source": [
        "# defining feature extractor that we want to use (SIFT)\n",
        "extractor = cv2.xfeatures2d.SIFT_create()\n",
        "\n",
        "# initialize lists where we will store *all* keypoints and descriptors\n",
        "keypoints = []\n",
        "descriptors = []\n",
        "\n",
        "for img in tqdm(bw_images):\n",
        "    # extract keypoints and descriptors for each image\n",
        "    img_keypoints, img_descriptors = extractor.detectAndCompute(img, None)\n",
        "    keypoints.append(img_keypoints)\n",
        "    descriptors.append(img_descriptors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ppRySWZlpRBQ"
      },
      "outputs": [],
      "source": [
        "print(f\"len before: {len(descriptors)}\")\n",
        "# initialize list to store idx values of records to drop\n",
        "to_drop = []\n",
        "for i, img_descriptors in enumerate(descriptors):\n",
        "    # if there are no descriptors, add record idx to drop list\n",
        "    if img_descriptors is None:\n",
        "        to_drop.append(i)\n",
        "\n",
        "print(f\"indexes: {to_drop}\")\n",
        "# delete from list in reverse order\n",
        "for i in sorted(to_drop, reverse=True):\n",
        "    del descriptors[i], keypoints[i]\n",
        "\n",
        "print(f\"len after: {len(descriptors)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qONT1nU_pRBS"
      },
      "outputs": [],
      "source": [
        "output_image = []\n",
        "for x in range(3):\n",
        "    output_image.append(cv2.drawKeypoints(bw_images[x], keypoints[x], 0, (255, 0, 0),\n",
        "                                 flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS))\n",
        "    plt.imshow(output_image[x], cmap='gray')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tnQctkAopRBS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# select the same numbers in each run\n",
        "np.random.seed(0)\n",
        "# select 1000 random image index values\n",
        "sample_idx = np.random.randint(0, len(data)+1, 50).tolist()\n",
        "len(sample_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwCIAa26pRBS"
      },
      "outputs": [],
      "source": [
        "# extract the sample from descriptors\n",
        "# (we don't need keypoints)\n",
        "descriptors_sample = []\n",
        "\n",
        "for n in tqdm(sample_idx):\n",
        "    descriptors_sample.append(np.array(descriptors[n]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2w14ilLQpRBT"
      },
      "outputs": [],
      "source": [
        "all_descriptors = []\n",
        "# extract image descriptor lists\n",
        "for img_descriptors in tqdm(descriptors_sample):\n",
        "    # extract specific descriptors within the image\n",
        "    for descriptor in img_descriptors:\n",
        "        all_descriptors.append(descriptor)\n",
        "# convert to single numpy array\n",
        "all_descriptors = np.stack(all_descriptors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TA_lOoMapRBT"
      },
      "outputs": [],
      "source": [
        "# check the shape\n",
        "all_descriptors.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgrkFJhapRBT"
      },
      "outputs": [],
      "source": [
        "# we can count the number of descriptors contained in descriptors to confirm\n",
        "count = []\n",
        "for img_descriptors in descriptors_sample:\n",
        "    count.append(len(img_descriptors))\n",
        "# here we can see the number of descriptors for the first five images\n",
        "print(f\"first five: {count[:5]}\")\n",
        "# and if we sum them all, we should see the 39893 from before\n",
        "print(f\"count all: {sum(count)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzWnVaCmpRBT"
      },
      "outputs": [],
      "source": [
        "# perform k-means clustering to build the codebook\n",
        "from scipy.cluster.vq import kmeans\n",
        "\n",
        "k = 200\n",
        "iters = 1\n",
        "codebook, variance = kmeans(all_descriptors, k, iters) # It takes approx. 2-3 minutes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GvytgkB0pRBT"
      },
      "outputs": [],
      "source": [
        "# vector quantization\n",
        "from scipy.cluster.vq import vq\n",
        "\n",
        "visual_words = []\n",
        "for img_descriptors in tqdm(descriptors):\n",
        "    # for each image, map each descriptor to the nearest codebook entry\n",
        "    img_visual_words, distance = vq(img_descriptors, codebook)\n",
        "    visual_words.append(img_visual_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxZKcnSupRBT"
      },
      "outputs": [],
      "source": [
        "# let's see what the visual words look like for image 0\n",
        "visual_words[0][:5], len(visual_words[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aAxwkfcFpRBU"
      },
      "outputs": [],
      "source": [
        "frequency_vectors = []\n",
        "for img_visual_words in tqdm(visual_words):\n",
        "    # create a frequency vector for each image\n",
        "    img_frequency_vector = np.zeros(k)\n",
        "    for word in img_visual_words:\n",
        "        img_frequency_vector[word] += 1\n",
        "    frequency_vectors.append(img_frequency_vector)\n",
        "# stack together in numpy array\n",
        "frequency_vectors = np.stack(frequency_vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0mp51dcpRBU"
      },
      "outputs": [],
      "source": [
        "frequency_vectors.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6mpVsGYpRBU"
      },
      "outputs": [],
      "source": [
        "# we know from above that ids 84, 22, 45, and 172 appear in image 0\n",
        "for i in [84,  22,  45, 172]:\n",
        "    print(f\"{i}: {frequency_vectors[0][i]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3PMvIdupRBU"
      },
      "outputs": [],
      "source": [
        "frequency_vectors[0][:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hg53GOmhpRBU"
      },
      "outputs": [],
      "source": [
        "plt.bar(list(range(k)), frequency_vectors[0])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fC4yhdKpRBU"
      },
      "outputs": [],
      "source": [
        "# N is the number of images, i.e. the size of the dataset\n",
        "N = 2659\n",
        "\n",
        "# df is the number of images that a visual word appears in\n",
        "# we calculate it by counting non-zero values as 1 and summing\n",
        "df = np.sum(frequency_vectors > 0, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahYJNkYhpRBU"
      },
      "outputs": [],
      "source": [
        "df.shape, df[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gw_uVttwpRBU"
      },
      "outputs": [],
      "source": [
        "idf = np.log(N/ df)\n",
        "idf.shape, idf[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1NNW2HwQpRBV"
      },
      "outputs": [],
      "source": [
        "tfidf_sat = frequency_vectors * idf\n",
        "tfidf_sat.shape, tfidf[0][:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aiao1wenpRBV"
      },
      "outputs": [],
      "source": [
        "plt.bar(list(range(k)), tfidf[0])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cd-PLA43pRBV"
      },
      "outputs": [],
      "source": [
        "search_i = 0\n",
        "\n",
        "plt.imshow(bw_images[search_i], cmap='gray')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WD-yLrV1pRBV"
      },
      "outputs": [],
      "source": [
        "# cosine similarity\n",
        "from numpy.linalg import norm\n",
        "\n",
        "a_sat = tfidf_sat[search_i]\n",
        "b_sat = tfidf_sat  # set search space to the full sample\n",
        "\n",
        "cosine_similarity = np.dot(a_sat, b_sat.T)/(norm(a_sat) * norm(b_sat, axis=1))\n",
        "print(\"Min cosine similarity:\", round(np.min(cosine_similarity),1))\n",
        "print(\"Max cosine similarity:\", np.max(cosine_similarity))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7is6nQkmpRBV"
      },
      "outputs": [],
      "source": [
        "cosine_similarity.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KN3LZ1WpRBV"
      },
      "outputs": [],
      "source": [
        "cosine_similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wICs3n62pRBV"
      },
      "outputs": [],
      "source": [
        "top_k = 5\n",
        "idx = np.argsort(-cosine_similarity)[:top_k]\n",
        "idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEqtRQxipRBW"
      },
      "outputs": [],
      "source": [
        "cosine_similarity[idx[0]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SH9OLMy6pRBW"
      },
      "outputs": [],
      "source": [
        "for i in idx:\n",
        "    print(f\"{i}: {round(cosine_similarity[i], 4)}\")\n",
        "    plt.imshow(bw_images[i], cmap='gray')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZOrNP_IpRBW"
      },
      "outputs": [],
      "source": [
        "def search(i: int, top_k: int = 5):\n",
        "    print(\"Search image:\")\n",
        "    # show the search image\n",
        "    plt.imshow(bw_images_g[i], cmap='gray')\n",
        "    plt.show()\n",
        "    print(\"-----------------------------------------------------\")\n",
        "    # get search image vector\n",
        "    a = tfidf[i]\n",
        "    # get the cosine distance for the search image `a`\n",
        "    cosine_similarity = np.dot(a, b_sat.T)/(norm(a) * norm(b_sat, axis=1))\n",
        "    # get the top k indices for most similar vecs\n",
        "    idx = np.argsort(-cosine_similarity)[:top_k]\n",
        "    # display the results\n",
        "    for i in idx:\n",
        "        print(f\"{i}: {round(cosine_similarity[i], 4)}\")\n",
        "        plt.imshow(bw_images[i], cmap='gray')\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prELivjipRBW"
      },
      "outputs": [],
      "source": [
        "search(121)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrjMRQElpRBW"
      },
      "outputs": [],
      "source": [
        "search(3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(3,7):\n",
        "  cv2_imshow(images_training[i])"
      ],
      "metadata": {
        "id": "zB2BgqNGpRBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8NL_qDYpRBW"
      },
      "outputs": [],
      "source": [
        "search(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##CVUSA"
      ],
      "metadata": {
        "id": "zhmuKdXDbNvn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python"
      ],
      "metadata": {
        "id": "uheNeH3hbVV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os"
      ],
      "metadata": {
        "id": "GF-22ZhEkGWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creo vettore immagini"
      ],
      "metadata": {
        "id": "4UWuNGCtnjyV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import os\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Definisci il percorso delle cartelle delle immagini\n",
        "aerial_folder = \"/content/CVUSA_subset/polarmap/normal\"\n",
        "ground_folder = \"/content/CVUSA_subset/streetview\"\n",
        "categories=[]\n",
        "# Carica le immagini ground\n",
        "ground_images = []\n",
        "for filename in sorted(os.listdir(ground_folder)):\n",
        "        image_path = os.path.join(ground_folder, filename)\n",
        "        ground_images.append(cv2.imread(image_path, cv2.IMREAD_GRAYSCALE))\n",
        "        categories.append(\"ground\")\n",
        "# Carica le immagini aerial\n",
        "aerial_images = []\n",
        "for filename in sorted(os.listdir(aerial_folder)):\n",
        "        image_path = os.path.join(aerial_folder, filename)\n",
        "        aerial_images.append(cv2.imread(image_path, cv2.IMREAD_GRAYSCALE))\n",
        "        categories.append(\"aerial\")"
      ],
      "metadata": {
        "id": "ltAD5QKkniCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv2_imshow(ground_images[0])"
      ],
      "metadata": {
        "id": "G8yPvjq1nvIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv2_imshow(aerial_images[0])"
      ],
      "metadata": {
        "id": "Z-T8Lgk7NL5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ground_images[0].shape)\n",
        "print(aerial_images[0].shape)"
      ],
      "metadata": {
        "id": "dsmBdpag2JFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SIFT"
      ],
      "metadata": {
        "id": "JlPrc2vTnlx_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "# Inizializza l'oggetto SIFT\n",
        "sift = cv2.SIFT_create()\n",
        "\n",
        "# Estrai i descrittori SIFT per tutte le immagini ground e aerial\n",
        "\n",
        "ground_descriptors = []\n",
        "kp_ground = []\n",
        "i=0\n",
        "for img in tqdm(ground_images, desc='Processing ground images'):\n",
        "    if(i<1000):\n",
        "      kp, des = sift.detectAndCompute(img, None)\n",
        "      ground_descriptors.append(des)\n",
        "      kp_ground.append(kp)\n",
        "      del kp,des\n",
        "    else: break\n",
        "    i+=1\n",
        "\n"
      ],
      "metadata": {
        "id": "HX9XTVWVkqxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aerial_descriptors = []\n",
        "kp_aerial = []\n",
        "i=0\n",
        "for img in tqdm(aerial_images, desc='Processing aerial images'):\n",
        "    if(i<1000):\n",
        "      kp, des = sift.detectAndCompute(img, None)\n",
        "      aerial_descriptors.append(des)\n",
        "      kp_aerial.append(kp)\n",
        "      del kp,des\n",
        "    else: break\n",
        "    i+=1\n"
      ],
      "metadata": {
        "id": "QOxPs-t9M9rn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"len before: {len(ground_descriptors)}\")\n",
        "# initialize list to store idx values of records to drop\n",
        "to_drop = []\n",
        "for i, img_descriptors in enumerate(ground_descriptors):\n",
        "    # if there are no descriptors, add record idx to drop list\n",
        "    if img_descriptors is None:\n",
        "        to_drop.append(i)\n",
        "\n",
        "print(f\"indexes: {to_drop}\")\n",
        "# delete from list in reverse order\n",
        "for i in sorted(to_drop, reverse=True):\n",
        "    del ground_descriptors[i], kp_ground[i]\n",
        "\n",
        "print(f\"len after: {len(ground_descriptors)}\")"
      ],
      "metadata": {
        "id": "4bvo70sfdirl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"len before: {len(aerial_descriptors)}\")\n",
        "# initialize list to store idx values of records to drop\n",
        "to_drop = []\n",
        "for i, img_descriptors in enumerate(aerial_descriptors):\n",
        "    # if there are no descriptors, add record idx to drop list\n",
        "    if img_descriptors is None:\n",
        "        to_drop.append(i)\n",
        "\n",
        "print(f\"indexes: {to_drop}\")\n",
        "# delete from list in reverse order\n",
        "for i in sorted(to_drop, reverse=True):\n",
        "    del aerial_descriptors[i], kp_aerial[i]\n",
        "\n",
        "print(f\"len after: {len(aerial_descriptors)}\")"
      ],
      "metadata": {
        "id": "wuPGkNMfdvfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_descriptors = []\n",
        "descriptors= []\n",
        "# extract image descriptor lists\n",
        "for img_descriptors in tqdm(ground_descriptors):\n",
        "    # extract specific descriptors within the image\n",
        "    for descriptor in img_descriptors:\n",
        "        all_descriptors.append(descriptor)\n",
        "\n",
        "for img_descriptors in tqdm(aerial_descriptors):\n",
        "    # extract specific descriptors within the image\n",
        "    for descriptor in img_descriptors:\n",
        "        all_descriptors.append(descriptor)\n",
        "\n",
        "\n",
        "# extract image descriptor lists\n",
        "for img_descriptors in tqdm(ground_descriptors):\n",
        "    # extract specific descriptors within the image\n",
        "        descriptors.append(img_descriptors)\n",
        "\n",
        "for img_descriptors in tqdm(aerial_descriptors):\n",
        "    # extract specific descriptors within the image\n",
        "        descriptors.append(img_descriptors)\n",
        "\n",
        "# convert to single numpy array\n",
        "all_descriptors = np.stack(all_descriptors)\n",
        "print(all_descriptors.shape)"
      ],
      "metadata": {
        "id": "OWcmy8C-M_mt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "output_image = []\n",
        "for x in range(3):\n",
        "    output_image.append(cv2.drawKeypoints(aerial_images[x], kp_aerial[x], 0, (255, 0, 0),\n",
        "                                 flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS))\n",
        "    plt.imshow(output_image[x], cmap='gray')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "2lw5b9x_o0xm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from scipy.cluster.vq import kmeans\n",
        "from numpy.linalg import norm\n",
        "num_clusters = 2\n",
        "codebook, variance = kmeans(all_descriptors, 200, 1)\n",
        "# vector quantization\n",
        "from scipy.cluster.vq import vq\n",
        "print(\"shape: \", all_descriptors.shape)\n",
        "print(\"shape: \", codebook.shape)\n",
        "visual_words = []\n",
        "for img_descriptors in tqdm(descriptors):\n",
        "    # for each image, map each descriptor to the nearest codebook entry\n",
        "    img_visual_words, distance = vq(img_descriptors, codebook)\n",
        "    visual_words.append(img_visual_words)\n",
        "\n"
      ],
      "metadata": {
        "id": "04Z86M6Fo0AD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frequency_vectors = []\n",
        "for img_visual_words in tqdm(visual_words):\n",
        "    # create a frequency vector for each image\n",
        "    img_frequency_vector = np.zeros(200)\n",
        "    for word in img_visual_words:\n",
        "        img_frequency_vector[word] += 1\n",
        "    frequency_vectors.append(img_frequency_vector)\n",
        "# stack together in numpy array\n",
        "frequency_vectors = np.stack(frequency_vectors)\n",
        "\n",
        "# N is the number of images, i.e. the size of the dataset\n",
        "N = 1998\n",
        "\n",
        "# df is the number of images that a visual word appears in\n",
        "# we calculate it by counting non-zero values as 1 and summing\n",
        "df = np.sum(frequency_vectors > 0, axis=0)\n",
        "idf = np.log(N/ df)\n",
        "tfidf = frequency_vectors * idf"
      ],
      "metadata": {
        "id": "kjsm5Z5y4Sy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search(i: int, category: str, top_k: int = 15):\n",
        "    print(\"Search image:\")\n",
        "    # show the search image\n",
        "    if category == 'ground':\n",
        "      plt.imshow(ground_images[i], cmap='gray')\n",
        "      plt.show()\n",
        "      a = tfidf[i]\n",
        "    else:\n",
        "      plt.imshow(aerial_images[i], cmap='gray')\n",
        "      plt.show()\n",
        "      a = tfidf[i+1000]\n",
        "    print(\"-----------------------------------------------------\")\n",
        "    print(tfidf.shape)\n",
        "\n",
        "\n",
        "    # Calculate cosine similarity with images of the opposite category\n",
        "    cosine_similarity = np.dot(a, tfidf.T) / (norm(a) * norm(tfidf, axis=1))\n",
        "\n",
        "    # Get the top k indices for most similar vectors\n",
        "    idx = np.argsort(-cosine_similarity)\n",
        "    idx_filtered=None\n",
        "    if category == 'ground':\n",
        "      idx_filtered = idx[idx >= 1000][:top_k]\n",
        "    else:\n",
        "      idx_filtered = idx[idx < 1000][:top_k]\n",
        "    print(idx_filtered)\n",
        "    # Display the results\n",
        "    for idx_i in idx_filtered:\n",
        "        print(f\"Similarity with image {idx_i} of category {categories[idx_i]}: {round(cosine_similarity[idx_i], 4)}\")\n",
        "        if category == 'ground':\n",
        "          plt.imshow(aerial_images[idx_i-1000], cmap='gray')\n",
        "          plt.show()\n",
        "        else:\n",
        "          plt.imshow(ground_images[idx_i], cmap='gray')\n",
        "          plt.show()\n"
      ],
      "metadata": {
        "id": "ueGvQmUgofRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1000):\n",
        "  print(\"id: \",i, np.dot(tfidf[i], tfidf[i+1000].T) / (norm(tfidf[i]) * norm(tfidf[i+1000])))"
      ],
      "metadata": {
        "id": "lIRob4Neigon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search(9,'ground')"
      ],
      "metadata": {
        "id": "7GNbDv_e5vEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.cluster import KMeans\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class ImageSimilarityCalculator:\n",
        "    def __init__(self, aerial_link, ground_link):\n",
        "        self.aerial_link = aerial_link\n",
        "        self.ground_link = ground_link\n",
        "        self.num_clusters = 200\n",
        "        self.sift = cv2.SIFT_create()\n",
        "\n",
        "    def load_images(self, folder):\n",
        "        images = []\n",
        "        for filename in sorted(os.listdir(folder)):\n",
        "            image_path = os.path.join(folder, filename)\n",
        "            images.append(cv2.imread(image_path, cv2.IMREAD_GRAYSCALE))\n",
        "        return images\n",
        "\n",
        "    def process_images(self, images):\n",
        "        descriptors = []\n",
        "        keypoints = []\n",
        "        for img in tqdm(images, desc='Processing images'):\n",
        "            kp, des = self.sift.detectAndCompute(img, None)\n",
        "            descriptors.append(des)\n",
        "            keypoints.append(kp)\n",
        "        return descriptors, keypoints\n",
        "\n",
        "    def remove_invalid_descriptors(self, descriptors, keypoints):\n",
        "        to_drop = []\n",
        "        for i, img_descriptors in enumerate(descriptors):\n",
        "            if img_descriptors is None:\n",
        "                to_drop.append(i)\n",
        "        for i in sorted(to_drop, reverse=True):\n",
        "            del descriptors[i], keypoints[i]\n",
        "        return descriptors, keypoints\n",
        "\n",
        "    def concatenate_descriptors(self, descriptors):\n",
        "        all_descriptors = np.concatenate(descriptors, axis=0)\n",
        "        return all_descriptors\n",
        "\n",
        "    def visualize_keypoints(self, images, keypoints):\n",
        "        output_images = []\n",
        "        for img, kp in zip(images, keypoints):\n",
        "            output_images.append(cv2.drawKeypoints(img, kp, 0, (255, 0, 0), flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS))\n",
        "            plt.imshow(output_images[-1], cmap='gray')\n",
        "            plt.show()\n",
        "\n",
        "    def calculate_similarity(self, tfidf_descriptors):\n",
        "        num_ground_images = len(self.ground_images)\n",
        "        num_aerial_images = len(self.aerial_images)\n",
        "        for i in range(num_ground_images):\n",
        "            for j in range(num_aerial_images):\n",
        "                similarity = cosine_similarity(tfidf_descriptors[i], tfidf_descriptors[j + num_ground_images])\n",
        "                print(f\"Similarity between ground image {i} and aerial image {j}:\", similarity)\n",
        "\n",
        "    def process(self):\n",
        "        # Load images\n",
        "        self.ground_images = self.load_images(self.ground_link)\n",
        "        self.aerial_images = self.load_images(self.aerial_link)\n",
        "\n",
        "        # Process images\n",
        "        self.ground_descriptors, self.kp_ground = self.process_images(self.ground_images)\n",
        "        self.aerial_descriptors, self.kp_aerial = self.process_images(self.aerial_images)\n",
        "\n",
        "        # Remove invalid descriptors\n",
        "        self.ground_descriptors, self.kp_ground = self.remove_invalid_descriptors(self.ground_descriptors, self.kp_ground)\n",
        "        self.aerial_descriptors, self.kp_aerial = self.remove_invalid_descriptors(self.aerial_descriptors, self.kp_aerial)\n",
        "\n",
        "        # Concatenate descriptors\n",
        "        all_ground_descriptors = self.concatenate_descriptors(self.ground_descriptors)\n",
        "        all_aerial_descriptors = self.concatenate_descriptors(self.aerial_descriptors)\n",
        "        all_descriptors = np.concatenate((all_ground_descriptors, all_aerial_descriptors), axis=0)\n",
        "\n",
        "        # Visualize keypoints\n",
        "        self.visualize_keypoints(self.aerial_images[:3], self.kp_aerial[:3])\n",
        "\n",
        "        # Utilize K-Means for clustering\n",
        "        kmeans = KMeans(n_clusters=self.num_clusters)\n",
        "        kmeans.fit(all_descriptors)\n",
        "        codebook = kmeans.cluster_centers_\n",
        "\n",
        "        # Calculate cluster labels\n",
        "        labels = kmeans.predict(all_descriptors)\n",
        "\n",
        "        # Calculate histograms\n",
        "        histograms = []\n",
        "        start_index = 0\n",
        "        for descriptors, images in [(self.ground_descriptors, self.ground_images), (self.aerial_descriptors, self.aerial_images)]:\n",
        "            for img in images:\n",
        "                end_index = start_index + len(self.sift.detectAndCompute(img, None)[1])\n",
        "                histogram = np.bincount(labels[start_index:end_index], minlength=self.num_clusters)\n",
        "                histograms.append(histogram)\n",
        "                start_index = end_index\n",
        "\n",
        "        # Calculate TF-IDF\n",
        "        tfidf_transformer = TfidfTransformer()\n",
        "        tfidf_descriptors = tfidf_transformer.fit_transform(histograms)\n",
        "\n",
        "        # Calculate similarity\n",
        "        self.calculate_similarity(tfidf_descriptors)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dEcdoVfyg2b2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aerial_link = \"/content/CVUSA_subset/polarmap/normal\"\n",
        "ground_link = \"/content/CVUSA_subset/streetview\"\n",
        "similarity_calculator = ImageSimilarityCalculator(aerial_link, ground_link)\n",
        "similarity_calculator.process()"
      ],
      "metadata": {
        "id": "VbgvzTXpjO-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Siamese Approach"
      ],
      "metadata": {
        "id": "Klmeyr0iD0D2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the lists"
      ],
      "metadata": {
        "id": "RdbHtBfKRxDk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Definisci il percorso delle cartelle delle immagini\n",
        "aerial_folder = \"/content/CVUSA_subset/polarmap/normal\"\n",
        "ground_folder = \"/content/CVUSA_subset/streetview\"\n",
        "categories=[]\n",
        "\n",
        "# Carica le immagini ground\n",
        "ground_images = []\n",
        "for filename in sorted(os.listdir(ground_folder)):\n",
        "    image_path = os.path.join(ground_folder, filename)\n",
        "    img = cv2.imread(image_path)\n",
        "    resized_img = cv2.resize(img, (256, 256))  # Ridimensiona l'immagine a 64x64\n",
        "    ground_images.append(resized_img)\n",
        "    categories.append(\"ground\")\n",
        "\n",
        "# Carica le immagini aerial\n",
        "aerial_images = []\n",
        "for filename in sorted(os.listdir(aerial_folder)):\n",
        "    image_path = os.path.join(aerial_folder, filename)\n",
        "    img = cv2.imread(image_path)\n",
        "    resized_img = cv2.resize(img, (256, 256))  # Ridimensiona l'immagine a 64x64\n",
        "    aerial_images.append(resized_img)\n",
        "    categories.append(\"aerial\")\n"
      ],
      "metadata": {
        "id": "ECgPqFVoRwnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aerial_images[0]"
      ],
      "metadata": {
        "id": "0uhksUJGhS1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Network"
      ],
      "metadata": {
        "id": "0axGT2PKHxPK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import pytorch_lightning as pl\n",
        "from torch.utils.data import random_split\n",
        "import logging\n",
        "from collections import OrderedDict\n",
        "\n",
        "from pytorch_lightning import LightningModule\n",
        "from pytorch_lightning import Trainer\n",
        "from torch import optim\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from pytorch_lightning.callbacks.progress import RichProgressBar\n",
        "\n",
        "# Define dataset class\n",
        "class ImagePairDataset(Dataset):\n",
        "    def __init__(self, ground_images, aerial_images, transform=None):\n",
        "        self.ground_images = ground_images\n",
        "        self.aerial_images = aerial_images\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ground_images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ground_image = self.ground_images[idx]\n",
        "        aerial_image = self.aerial_images[idx]\n",
        "        if self.transform:\n",
        "            ground_image = self.transform(ground_image)\n",
        "            aerial_image = self.transform(aerial_image)\n",
        "        return ground_image, aerial_image\n",
        "\n",
        "\n",
        "# Define Siamese Network\n",
        "class SiameseNetwork(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super(SiameseNetwork, self).__init__()\n",
        "        # Define convolutional layers for feature extraction\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=0)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=0)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=0)\n",
        "        self.fc1 = nn.Linear(230400, 256)  # Assuming input image size is 64x64\n",
        "\n",
        "    def forward(self, x): #prima forward_once\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv3(x), 2))\n",
        "\n",
        "        x_flat = torch.flatten(x, 1)  # Appiattisce il tensore, mantenendo la dimensione del batch\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return x\n",
        "\n",
        "    # def forward(self, input1, input2):\n",
        "    #     output1 = self.forward_once(input1)\n",
        "    #     output2 = self.forward_once(input2)\n",
        "    #     return output1, output2\n",
        "\n",
        "class WeightedSoftMarginTripletLoss(pl.LightningModule):\n",
        "    def __init__(self,ImagePairDataset, loss_weight=10.0):\n",
        "        super(WeightedSoftMarginTripletLoss, self).__init__()\n",
        "\n",
        "        self.ImagePairDataset = ImagePairDataset\n",
        "        # Define the sizes for train, validation, and test sets\n",
        "        train_size = int(0.7 * len(ImagePairDataset))  # 70% for training\n",
        "\n",
        "        val_size = int(0.15 * len(ImagePairDataset))   # 15% for validation\n",
        "\n",
        "        test_size = len(ImagePairDataset) - train_size - val_size  # Remaining for testing\n",
        "\n",
        "        # Use random_split to split the dataset\n",
        "        self.datatrain, self.dataval, self.datatest = random_split(ImagePairDataset, [train_size, val_size, test_size])\n",
        "\n",
        "        self.SiameseNetwork = SiameseNetwork()\n",
        "        self.loss_weight = loss_weight\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        grd_global = self.SiameseNetwork.forward(x1)\n",
        "        sat_global = self.SiameseNetwork.forward(x2)\n",
        "\n",
        "        dist_matrix = 2 - 2 * torch.matmul(sat_global, grd_global.t())\n",
        "\n",
        "        pos_dist = torch.diag(dist_matrix)\n",
        "\n",
        "        batch_size = sat_global.size(0)\n",
        "\n",
        "        pair_n = batch_size * (batch_size - 1.0)\n",
        "\n",
        "        # ground to satellite\n",
        "        triplet_dist_g2s = pos_dist - dist_matrix\n",
        "\n",
        "        # satellite to ground\n",
        "        triplet_dist_s2g = pos_dist.unsqueeze(1) - dist_matrix\n",
        "\n",
        "\n",
        "        return triplet_dist_g2s , triplet_dist_s2g , pair_n ,grd_global, sat_global\n",
        "\n",
        "\n",
        "    def loss(self,triplet_dist_g2s , triplet_dist_s2g , pair_n ):\n",
        "        loss_g2s = torch.sum(torch.log(1 + torch.exp(triplet_dist_g2s * self.loss_weight))) / pair_n\n",
        "        loss_s2g = torch.sum(torch.log(1 + torch.exp(triplet_dist_s2g * self.loss_weight))) / pair_n\n",
        "        loss = (loss_g2s + loss_s2g) / 2.0\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def validate_topk(self, dist_array, topK):\n",
        "        accuracy = 0.0\n",
        "        data_amount = 0.0\n",
        "\n",
        "        for i in range(dist_array.shape[0]):\n",
        "            gt_dist = dist_array[i, i]\n",
        "            prediction = np.sum(dist_array[i, :] < gt_dist)\n",
        "            if prediction < topK:\n",
        "                accuracy += 1.0\n",
        "            data_amount += 1.0\n",
        "        accuracy /= data_amount\n",
        "\n",
        "        return accuracy\n",
        "\n",
        "    def validate(self, grd_descriptor, sat_descriptor):\n",
        "        accuracy = 0.0\n",
        "        data_amount = 0.0\n",
        "        dist_array = 2 - 2 * np.matmul(sat_descriptor.cpu(), np.transpose(grd_descriptor.cpu()))\n",
        "\n",
        "        top1_percent = int(dist_array.shape[0] * 0.01) + 1\n",
        "        for i in range(dist_array.shape[0]):\n",
        "            gt_dist = dist_array[i, i]\n",
        "            prediction = torch.sum(dist_array[:, i].cpu().lt(gt_dist.cpu()))\n",
        "            if prediction < top1_percent:\n",
        "                accuracy += 1.0\n",
        "            data_amount += 1.0\n",
        "        accuracy /= data_amount\n",
        "\n",
        "        return accuracy\n",
        "\n",
        "\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "      grd, sat = batch\n",
        "      dist_g2s , dist_s2g , pair_n , grd_global , sat_global = self.forward(grd ,sat)\n",
        "      loss_val = self.loss(dist_g2s , dist_s2g , pair_n)\n",
        "      self.log('train_loss', loss_val, on_step=False, on_epoch=True, prog_bar=True )\n",
        "      tqdm_dict = OrderedDict({\"loss_train\": loss_val})\n",
        "      output = {\"loss\": loss_val, \"progress_bar\": tqdm_dict, \"log\": tqdm_dict}\n",
        "      return output\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        grd, sat = batch\n",
        "        dist_g2s, dist_s2g, pair_n, grd_global , sat_global = self.forward(grd, sat)\n",
        "        loss_val = self.loss(dist_g2s, dist_s2g, pair_n)\n",
        "        accuracy_val = self.validate(grd_global, sat_global)  # Calcola l'accuratezza\n",
        "        self.log('val_loss', loss_val, on_step=False, on_epoch=True, prog_bar=True )\n",
        "        self.log('val_accuracy', accuracy_val, on_step=False, on_epoch=True, prog_bar=True )\n",
        "        output= {\"val_loss\": loss_val, \"val_accuracy\": accuracy_val}\n",
        "        return output\n",
        "\n",
        "\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        grd, sat = batch\n",
        "        dist_g2s, dist_s2g, pair_n, grd_global, sat_global = self.forward(grd, sat)\n",
        "        loss_val = self.loss(dist_g2s, dist_s2g, pair_n)\n",
        "        accuracy_val = self.validate(grd_global, sat_global)  # Calcola l'accuratezza\n",
        "        self.log('test_loss', loss_val, on_step=False, on_epoch=True, prog_bar=True )\n",
        "        self.log('test_accuracy', accuracy_val, on_step=False, on_epoch=True, prog_bar=True )\n",
        "        return {\"test_loss\": loss_val, \"test_accuracy\": accuracy_val}\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        \"\"\"\n",
        "        return whatever optimizers we want here\n",
        "        :return: list of optimizers\n",
        "        \"\"\"\n",
        "        optimizer = optim.SGD(self.parameters(),\n",
        "                             lr=0.01, momentum=0.90)\n",
        "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer,\n",
        "                                                         T_max=10)\n",
        "        return [optimizer], [scheduler]\n",
        "\n",
        "    def __dataloader(self, train, dataset):\n",
        "        # when using multi-node (ddp) we need to add the  datasampler\n",
        "        train_sampler = None\n",
        "        batch_size = 80\n",
        "\n",
        "        should_shuffle = train and train_sampler is None\n",
        "        loader = DataLoader(\n",
        "            dataset=dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=should_shuffle,\n",
        "            sampler=train_sampler,\n",
        "            num_workers=0,\n",
        "            drop_last=True\n",
        "        )\n",
        "\n",
        "        return loader\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        logging.info('training data loader called')\n",
        "        return self.__dataloader(train=True, dataset=self.datatrain)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        logging.info('val data loader called')\n",
        "        return self.__dataloader(train=False, dataset=self.dataval)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        logging.info('val data loader called')\n",
        "        return self.__dataloader(train=False, dataset=self.datatest)\n",
        "\n"
      ],
      "metadata": {
        "id": "orydrr8-mkG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data setup\n",
        "# Assuming you have ground_images and aerial_images as lists of image paths\n",
        "# Assuming you have defined appropriate transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "# Create a RichProgressBar callback\n",
        "progress_bar = RichProgressBar()\n",
        "\n",
        "# Initialize dataset and dataloader\n",
        "dataset = ImagePairDataset(ground_images, aerial_images, transform=transform)\n",
        "\n",
        "# Initialize Siamese Network, Triplet Loss, and LightningModule\n",
        "\n",
        "model = WeightedSoftMarginTripletLoss(dataset)\n",
        "\n",
        "trainer = Trainer(max_epochs=20 , callbacks=[progress_bar])\n",
        "\n",
        "trainer.fit(model)\n"
      ],
      "metadata": {
        "id": "U80tvd8QclPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.test(model)"
      ],
      "metadata": {
        "id": "6ZtjHh4Evvfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tensorboard --logdir=logs\n"
      ],
      "metadata": {
        "id": "tn-K6-hxw6gn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "FTH52kmbCo0d",
        "wdMTOVNGOdLR",
        "n8LvyWcZT6Sy",
        "YHv90khTDZ6D",
        "jOInv_qCa9c4",
        "1W_LVSIQpRBJ",
        "zhmuKdXDbNvn",
        "Klmeyr0iD0D2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ee6f2375611e4cdbb7c5a3dd2162a0b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f8a158b442764544b15783f465418a5d",
              "IPY_MODEL_4fd876bd8b7a4231831cb3a415497c66",
              "IPY_MODEL_bf7ae107b04d4d3b8340554a8eac3843"
            ],
            "layout": "IPY_MODEL_5c9357bae738484398db5818e6e0d83e"
          }
        },
        "f8a158b442764544b15783f465418a5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af5ccafa22344e83b55345382694266c",
            "placeholder": "​",
            "style": "IPY_MODEL_9500a11cec4d4fda8abc4b45d33ec8d3",
            "value": "Validation DataLoader 0:   3%"
          }
        },
        "4fd876bd8b7a4231831cb3a415497c66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29231e44ba814452a67ce34f51c9eb53",
            "max": 111,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5fb8b25402644115a453d947a1d21b8f",
            "value": 3
          }
        },
        "bf7ae107b04d4d3b8340554a8eac3843": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bdfc4834086c4061be8b830d1932a5bb",
            "placeholder": "​",
            "style": "IPY_MODEL_b4765129894d460fbf95ae643fa306d2",
            "value": " 3/111 [00:37&lt;22:25,  0.08it/s]"
          }
        },
        "5c9357bae738484398db5818e6e0d83e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "af5ccafa22344e83b55345382694266c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9500a11cec4d4fda8abc4b45d33ec8d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "29231e44ba814452a67ce34f51c9eb53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5fb8b25402644115a453d947a1d21b8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bdfc4834086c4061be8b830d1932a5bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4765129894d460fbf95ae643fa306d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}